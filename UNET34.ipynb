{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b8c18e1d49e580b84b194084a08d59e2a497b54"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f0a329467823b3dc6d5f86b79f55500cc44acc8"
   },
   "source": [
    "There are two most obvious network architectures to approach this competition: U-net and SSD. Each of them has pros and cons. In particular, U-net provides a relatively simple way to solve the competition challenge using image segmentation. However, this competition requires prediction of an individual mask for each ship rather than one mask for entire image. Therefore, some creative postprocessing may be needed, especially to separate ships with overlapping masks, if it is even possible. Another drawback is that the data is labeled with using pixelized bounding boxes rather than real ship masks, therefore the score of U-net based models is lowered. Meanwhile, implementation of SSD requires usage of rotating bounding boxes (https://arxiv.org/pdf/1711.09405.pdf) that is not common and, therefore, would take additional efforts for development of the model and the corresponding loss function. In addition, bounding boxes are not provided in this competition and must be generated based on the pixel masks. Nevertheless, this approach is expected to provide higher score than U-net, especially since the data is labeled based on pixelized bounding boxes (I expect organizers used SSD with rotating bounding boxes to label train and test data).\n",
    "Since the first approach is more straightforward, I'll begin with presenting a kernel about U-net. In this post I will describe how to use pretrained ResNet34 to build a high accuracy image segmentation model. In particular, after training only a decoder for 1 epoch (15 min) on 256x256 rescaled images, the dice coefficient reaches ~0.8 (IoU ~0.67) that significantly outperforms all publicly available models posted so far in this competition. After training the entire model for 6 additional epochs with learning rate annealing, the dice coefficient reaches ~0.86 (IoU ~0.75). Due to the kernel run time limit, the model is further trained only for two epochs on 384x384 (dice ~0.87) followed by one epoch on 768x768 images. In an independent run I trained a model on 384x384 images for 12 epochs that boosted dice to 0.89 followed by training on full resolution images that increased dice further to 0.905."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai==0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/6d/9d0d6e17a78b0598d5e8c49a0d03ffc7ff265ae62eca3e2345fab14edb9b/fastai-0.7.0-py3-none-any.whl (112kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 1.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ptyprocess in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.5.2)\n",
      "Collecting torchtext (from fastai==0.7.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/bc/b28b9efb4653c03e597ed207264eea45862b5260f48e9f010b5068d64db1/torchtext-0.3.1-py3-none-any.whl (62kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.5MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (1.14.3)\n",
      "Requirement already satisfied: MarkupSafe in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (1.0)\n",
      "Requirement already satisfied: PyYAML in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (3.12)\n",
      "Requirement already satisfied: seaborn in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.8.1)\n",
      "Requirement already satisfied: pytz in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (2018.4)\n",
      "Requirement already satisfied: pickleshare in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.7.4)\n",
      "Requirement already satisfied: pandas in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.23.0)\n",
      "Requirement already satisfied: matplotlib in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (2.2.2)\n",
      "Requirement already satisfied: traitlets in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (4.3.2)\n",
      "Requirement already satisfied: Pygments in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (2.2.0)\n",
      "Requirement already satisfied: pyparsing in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (2.2.0)\n",
      "Requirement already satisfied: jupyter in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (3.2.1)\n",
      "Requirement already satisfied: testpath in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.3.1)\n",
      "Requirement already satisfied: jsonschema in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (2.6.0)\n",
      "Collecting bcolz (from fastai==0.7.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 2.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting feather-format (from fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/08/55/940b97cc6f19a19f5dab9efef2f68a0ce43a7632f858b272391f0b851a7e/feather-format-0.4.0.tar.gz\n",
      "Requirement already satisfied: ipython in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (6.4.0)\n",
      "Requirement already satisfied: Pillow in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (5.1.0)\n",
      "Requirement already satisfied: simplegeneric in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.8.1)\n",
      "Requirement already satisfied: webencodings in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.5.1)\n",
      "Collecting graphviz (from fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/e2/ef2581b5b86625657afd32030f90cf2717456c1d2b711ba074bf007c0f1a/graphviz-0.10.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Jinja2 in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (2.10)\n",
      "Requirement already satisfied: opencv-python in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (3.4.2.17)\n",
      "Requirement already satisfied: html5lib in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (1.0.1)\n",
      "Requirement already satisfied: certifi in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (2018.4.16)\n",
      "Requirement already satisfied: ipykernel in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (4.8.2)\n",
      "Requirement already satisfied: bleach in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (2.1.3)\n",
      "Requirement already satisfied: cycler in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.10.0)\n",
      "Collecting torch<0.4 (from fastai==0.7.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/a5/e8b50b55b1abac9f1e3346c4242f1e42a82d368a8442cbd50c532922f6c4/torch-0.3.1-cp36-cp36m-manylinux1_x86_64.whl (496.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 496.4MB 138kB/s ta 0:00:011    44% |██████████████▍                 | 223.1MB 2.0MB/s eta 0:02:15    62% |████████████████████            | 311.2MB 1.9MB/s eta 0:01:38\n",
      "\u001b[?25hRequirement already satisfied: decorator in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (4.3.0)\n",
      "Requirement already satisfied: tqdm in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (4.28.1)\n",
      "Requirement already satisfied: torchvision in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.2.1)\n",
      "Requirement already satisfied: ipywidgets in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (7.2.1)\n",
      "Collecting sklearn-pandas (from fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/9c/c94f46b40b86d2c77c46c4c1b858fc66c117b4390665eca28f2e0812db45/sklearn_pandas-1.7.0-py2.py3-none-any.whl\n",
      "Collecting pandas-summary (from fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/97/55/ea54109a4e7a8e7342bdf23e9382c858224263d984b0d95610568e564f59/pandas_summary-0.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: entrypoints in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.2.3)\n",
      "Requirement already satisfied: ipython-genutils in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.2.0)\n",
      "Collecting isoweek (from fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/d4/fe7e2637975c476734fcbf53776e650a29680194eb0dd21dbdc020ca92de/isoweek-1.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jedi in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.12.0)\n",
      "Collecting plotnine (from fastai==0.7.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/02/b171c828560aea3a5da1efda464230dac3ef4f4834b88e0bd52ad14a08f0/plotnine-0.5.1-py2.py3-none-any.whl (3.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.6MB 2.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (2.7.3)\n",
      "Requirement already satisfied: pyzmq in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (17.0.0)\n",
      "Requirement already satisfied: scipy in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (1.1.0)\n",
      "Requirement already satisfied: tornado in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (5.0.2)\n",
      "Requirement already satisfied: wcwidth in /home/sophie/anaconda3/lib/python3.6/site-packages (from fastai==0.7.0) (0.1.7)\n",
      "Requirement already satisfied: requests in /home/sophie/anaconda3/lib/python3.6/site-packages (from torchtext->fastai==0.7.0) (2.18.4)\n",
      "Requirement already satisfied: six>=1.10 in /home/sophie/anaconda3/lib/python3.6/site-packages (from matplotlib->fastai==0.7.0) (1.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sophie/anaconda3/lib/python3.6/site-packages (from matplotlib->fastai==0.7.0) (1.0.1)\n",
      "Requirement already satisfied: notebook in /home/sophie/anaconda3/lib/python3.6/site-packages (from jupyter->fastai==0.7.0) (5.5.0)\n",
      "Requirement already satisfied: qtconsole in /home/sophie/anaconda3/lib/python3.6/site-packages (from jupyter->fastai==0.7.0) (4.3.1)\n",
      "Requirement already satisfied: jupyter-console in /home/sophie/anaconda3/lib/python3.6/site-packages (from jupyter->fastai==0.7.0) (5.2.0)\n",
      "Requirement already satisfied: nbconvert in /home/sophie/anaconda3/lib/python3.6/site-packages (from jupyter->fastai==0.7.0) (5.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow>=0.4.0 (from feather-format->fastai==0.7.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/94/23135312f97b20d6457294606fb70fad43ef93b7bffe567088ebe3623703/pyarrow-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (11.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 11.6MB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pexpect; sys_platform != \"win32\" in /home/sophie/anaconda3/lib/python3.6/site-packages (from ipython->fastai==0.7.0) (4.5.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/sophie/anaconda3/lib/python3.6/site-packages (from ipython->fastai==0.7.0) (39.1.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /home/sophie/anaconda3/lib/python3.6/site-packages (from ipython->fastai==0.7.0) (1.0.15)\n",
      "Requirement already satisfied: backcall in /home/sophie/anaconda3/lib/python3.6/site-packages (from ipython->fastai==0.7.0) (0.1.0)\n",
      "Requirement already satisfied: jupyter_client in /home/sophie/anaconda3/lib/python3.6/site-packages (from ipykernel->fastai==0.7.0) (5.2.3)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/sophie/anaconda3/lib/python3.6/site-packages (from ipywidgets->fastai==0.7.0) (4.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.15.0 in /home/sophie/anaconda3/lib/python3.6/site-packages (from sklearn-pandas->fastai==0.7.0) (0.19.1)\n",
      "Requirement already satisfied: parso>=0.2.0 in /home/sophie/anaconda3/lib/python3.6/site-packages (from jedi->fastai==0.7.0) (0.2.0)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /home/sophie/anaconda3/lib/python3.6/site-packages (from plotnine->fastai==0.7.0) (0.5.0)\n",
      "Collecting mizani>=0.5.2 (from plotnine->fastai==0.7.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/3a/1d1c5563b6aeb5fffda694b70d649a0f728a112b79a66b85a6af4814a643/mizani-0.5.2-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting descartes>=1.1.0 (from plotnine->fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/b6/1ed2eb03989ae574584664985367ba70cd9cf8b32ee8cad0e8aaeac819f3/descartes-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: statsmodels>=0.8.0 in /home/sophie/anaconda3/lib/python3.6/site-packages (from plotnine->fastai==0.7.0) (0.9.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/sophie/anaconda3/lib/python3.6/site-packages (from requests->torchtext->fastai==0.7.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/sophie/anaconda3/lib/python3.6/site-packages (from requests->torchtext->fastai==0.7.0) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/sophie/anaconda3/lib/python3.6/site-packages (from requests->torchtext->fastai==0.7.0) (1.22)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /home/sophie/anaconda3/lib/python3.6/site-packages (from notebook->jupyter->fastai==0.7.0) (0.8.1)\n",
      "Requirement already satisfied: Send2Trash in /home/sophie/anaconda3/lib/python3.6/site-packages (from notebook->jupyter->fastai==0.7.0) (1.5.0)\n",
      "Requirement already satisfied: jupyter-core>=4.4.0 in /home/sophie/anaconda3/lib/python3.6/site-packages (from notebook->jupyter->fastai==0.7.0) (4.4.0)\n",
      "Requirement already satisfied: mistune>=0.7.4 in /home/sophie/anaconda3/lib/python3.6/site-packages (from nbconvert->jupyter->fastai==0.7.0) (0.8.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/sophie/anaconda3/lib/python3.6/site-packages (from nbconvert->jupyter->fastai==0.7.0) (1.4.2)\n",
      "Collecting palettable (from mizani>=0.5.2->plotnine->fastai==0.7.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/8a/84537c0354f0d1f03bf644b71bf8e0a50db9c1294181905721a5f3efbf66/palettable-3.1.1-py2.py3-none-any.whl (77kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 2.9MB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: bcolz, feather-format\n",
      "  Running setup.py bdist_wheel for bcolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/sophie/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
      "  Running setup.py bdist_wheel for feather-format ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/sophie/.cache/pip/wheels/85/7d/12/2dfa5c0195f921ac935f5e8f27deada74972edc0ae9988a9c1\n",
      "Successfully built bcolz feather-format\n",
      "\u001b[31mmizani 0.5.2 has requirement pandas>=0.23.4, but you'll have pandas 0.23.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mplotnine 0.5.1 has requirement matplotlib>=3.0.0, but you'll have matplotlib 2.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mplotnine 0.5.1 has requirement pandas>=0.23.4, but you'll have pandas 0.23.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchtext, bcolz, pyarrow, feather-format, graphviz, sklearn-pandas, pandas-summary, isoweek, palettable, mizani, descartes, plotnine, fastai\n",
      "  Found existing installation: torch 1.0.0.dev20181130\n",
      "    Uninstalling torch-1.0.0.dev20181130:\n",
      "      Successfully uninstalled torch-1.0.0.dev20181130\n",
      "  Found existing installation: fastai 1.0.30\n",
      "    Uninstalling fastai-1.0.30:\n",
      "      Successfully uninstalled fastai-1.0.30\n",
      "Successfully installed bcolz-1.2.1 descartes-1.1.0 fastai-1.0.6 feather-format-0.4.0 graphviz-0.10.1 isoweek-1.3.3 mizani-0.5.2 palettable-3.1.1 pandas-summary-0.0.5 plotnine-0.5.1 pyarrow-0.11.1 sklearn-pandas-1.7.0 torch-0.4.1 torchtext-0.3.1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-058ddedb303f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install fastai==0.7.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_learner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/conv_learner.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch_imports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menum\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntEnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mscale_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \"\"\" Scales the image so that the smallest axis is of size targ.\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ddd71a95c05902b29ce4e3768a8127c2a0b7098d"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b93d5422dfd31f43df9cdd3e301547cc95f25980",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = './'\n",
    "TRAIN = '../input/airbus-ship-detection/train/'\n",
    "TEST = '../input/airbus-ship-detection/test/'\n",
    "SEGMENTATION = '../input/airbus-ship-detection/train_ship_segmentations.csv'\n",
    "PRETRAINED = '../input/fine-tuning-resnet34-on-ship-detection/models/Resnet34_lable_256_1.h5'\n",
    "exclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n",
    "                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n",
    "                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n",
    "                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] #corrupted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9a72e076dc04cf1786edae26a1d2f15ec3de234a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nw = 2   #number of workers for data loader\n",
    "arch = resnet34 #specify target architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49199ea17e9e9ba9893c10d06c1fb419e22aeb1b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_names = [f for f in os.listdir(TRAIN)]\n",
    "test_names = [f for f in os.listdir(TEST)]\n",
    "for el in exclude_list:\n",
    "    if(el in train_names): train_names.remove(el)\n",
    "    if(el in test_names): test_names.remove(el)\n",
    "#5% of data in the validation set is sufficient for model evaluation\n",
    "tr_n, val_n = train_test_split(train_names, test_size=0.05, random_state=42)\n",
    "segmentation_df = pd.read_csv(os.path.join(PATH, SEGMENTATION)).set_index('ImageId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b95102ddd563191f9b15eb115f736e93c837d080"
   },
   "source": [
    "One of the challenges of this competition is strong data unbalance. Even if only images with ships are considered, the ratio of mask pixels to the total number of pixels is ~1:1000. If images with no ships are included, this ratio goes to ~1:10000, which is quite tough to handle. Therefore, I drop all images without ships, that makes the training set more balanced and also reduces the time per each epoch almost by 4 times. In an independent run, when the dice of my model reached 0.895, I ran it on images without ships and identified ~3600 false positive predictions out ~70k images. The incorrectly predicted images were incorporated to the training set as negative examples, and training was continued. The problem of false positive predictions can be further mitigated by stacking U-net model with a classification model predicting if ships are present in a particular image (https://www.kaggle.com/iafoss/fine-tuning-resnet34-on-ship-detection - ~98% accuracy). \n",
    "I also noticed that in some kernels the dataset is tried to be balanced by keeping approximately the same number of images with 0, 1, 2, etc. ships. However, this strategy would be effective for such task as ship counting rather than training U-net or SSD.  One possible way to balance the dataset is creative cropping the images that keeps approximately the same number of pixels corresponding to a ship or something else. However, I doubt that such approach will effective in this competition. Therefore, a special loss function must be used to mitigate the data unbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a3d8c70e03964738322ca99084f088adc1c5f3e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_empty(names):\n",
    "    return [name for name in names \n",
    "            if(type(segmentation_df.loc[name]['EncodedPixels']) != float)]\n",
    "\n",
    "tr_n = cut_empty(tr_n)\n",
    "val_n = cut_empty(val_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "505f803555b8d9d0378a73d227da4b8174f2086d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mask(img_id, df):\n",
    "    shape = (768,768)\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    masks = df.loc[img_id]['EncodedPixels']\n",
    "    if(type(masks) == float): return img.reshape(shape)\n",
    "    if(type(masks) == str): masks = [masks]\n",
    "    for mask in masks:\n",
    "        s = mask.split()\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i]) - 1\n",
    "            length = int(s[2*i+1])\n",
    "            img[start:start+length] = 1\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f67a326e1269bffca392fcf4ac10bf4a532ca56b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class pdFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, path, transform):\n",
    "        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n",
    "        super().__init__(fnames, transform, path)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        img = open_image(os.path.join(self.path, self.fnames[i]))\n",
    "        if self.sz == 768: return img \n",
    "        else: return cv2.resize(img, (self.sz, self.sz))\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        mask = np.zeros((768,768), dtype=np.uint8) if (self.path == TEST) \\\n",
    "            else get_mask(self.fnames[i], self.segmentation_df)\n",
    "        img = Image.fromarray(mask).resize((self.sz, self.sz)).convert('RGB')\n",
    "        return np.array(img).astype(np.float32)\n",
    "    \n",
    "    def get_c(self): return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f51da2928183042aa99adfe9d1da494f6c624e57"
   },
   "source": [
    "The carrently availible on kaggle version of fastai has a bug in RandomLighting data agmentation class. It would be nice if kaggle updated fastai version to the last one, where this and other bugs are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "649601eb0bd110b02f15349cc9d26d60ddeee953",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomLighting(Transform):\n",
    "    def __init__(self, b, c, tfm_y=TfmType.NO):\n",
    "        super().__init__(tfm_y)\n",
    "        self.b,self.c = b,c\n",
    "\n",
    "    def set_state(self):\n",
    "        self.store.b_rand = rand0(self.b)\n",
    "        self.store.c_rand = rand0(self.c)\n",
    "\n",
    "    def do_transform(self, x, is_y):\n",
    "        if is_y and self.tfm_y != TfmType.PIXEL: return x  #add this line to fix the bug\n",
    "        b = self.store.b_rand\n",
    "        c = self.store.c_rand\n",
    "        c = -1/(c-1) if c<0 else c+1\n",
    "        x = lighting(x, b, c)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "835b75b3508f418bac53ee6725694af208e1f28c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    #data augmentation\n",
    "    aug_tfms = [RandomRotate(20, tfm_y=TfmType.CLASS),\n",
    "                RandomDihedral(tfm_y=TfmType.CLASS),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.CLASS)]\n",
    "    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS, \n",
    "                aug_tfms=aug_tfms)\n",
    "    tr_names = tr_n if (len(tr_n)%bs == 0) else tr_n[:-(len(tr_n)%bs)] #cut incomplete batch\n",
    "    ds = ImageData.get_ds(pdFilesDataset, (tr_names,TRAIN), \n",
    "                (val_n,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    md.is_multi = False\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b77a79e1e1e3f4c8a9e7120feb9a843b41492f8",
    "collapsed": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc66537bd5382d9e874cfa8c0616965e8d9f5f7d"
   },
   "source": [
    "The model used in this kernel is inspired by a Carvana example from FastAI course (http://course.fast.ai/index.html). It is composed of a ResNet34 based encoder and a simple upsampling decoder. Similar to the original U-net, skip connections are added between encoder and decoder to facilitate the information flow at different detalization levels. Meanwhile, using a pretrained ResNet34 model allows us to have a powerful encoder capable of handling elaborated feature, in comparison with the original U-net, without a risk of overfitting and necessity of training a big model from scratch. The total capacity of the model is ~21M parameters. Before using, the original ResNet34 model was further fine-tuned on ship/no-ship classification task (https://www.kaggle.com/iafoss/fine-tuning-resnet34-on-ship-detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bfefb757e1d40da7d8761c824170214a4bec08b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut,lr_cut = model_meta[arch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b0649b17721919f475a99f41f47f09c4fcc41d6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base():                   #load ResNet34 model\n",
    "    layers = cut_model(arch(True), cut)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def load_pretrained(model, path): #load a model pretrained on ship/no-ship classification\n",
    "    weights = torch.load(PRETRAINED, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77ca73006aec0c9dfe4be6deb2d6cf524840cd62",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p))\n",
    "\n",
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()\n",
    "    \n",
    "class Unet34(nn.Module):\n",
    "    def __init__(self, rn):\n",
    "        super().__init__()\n",
    "        self.rn = rn\n",
    "        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x)\n",
    "        return x[:,0]\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()\n",
    "            \n",
    "class UnetModel():\n",
    "    def __init__(self,model,name='Unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n",
    "        return lgs + [children(self.model)[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c71a3af2a83b1122902a3f39e6fda1af5afbb99",
    "collapsed": true
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19d0fe1a3e00cea135163b5540338e2744615c3d"
   },
   "source": [
    "Loss function is one of the most crucial parts of the completion. Due to strong data unbalance, simple loss functions, such as Binary Cross-Entropy loss, do not really work. Soft dice loss can be helpful since it boosts prediction of correct masks, but it leads to unstable training. Winners of image segmentation challenges typically combine BCE loss with dice (http://blog.kaggle.com/2017/12/22/carvana-image-masking-first-place-interview/). Similar loss function is used in publicly available models in this completion. I would agree that this combined loss function works perfectly for Carvana completion, where the number of pixels in the mask is about half of the total number of pixels. However, 1:1000 pixel unbalance deteriorates training with BCE. \n",
    "If one tries to recall what is the loss function that should be used for strongly unbalanced data set, it is focal loss (https://arxiv.org/pdf/1708.02002.pdf), which revolutionized one stage object localization method in 2017. This loss function demonstrates amazing results on datasets with unbalance level 1:10-1000. In addition to focal loss, I include -log(soft dice loss). Log is important in the convex of the current competition since it boosts the loss for the cases when objects are not detected correctly and dice is close to zero. It allows to avoid false negative predictions or completely incorrect masks for images with one ship (the major part of the training set). Also, since the loss for such objects is very high, the model more effectively incorporates the knowledge about such objects and handles them even in images with multiple ships. To bring two losses to similar scale, focal loss is multiplied by 10. The implementation of focal loss is borrowed from https://becominghuman.ai/investigating-focal-and-dice-loss-for-the-kaggle-2018-data-science-bowl-65fb9af4f36c ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8de4222fd28596dbae5d55f8172ac28271f678f2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "456acf8ffe7b80cd94eb73bfaeb6b1061bbb44c7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1cfb00a50821861c69534ee7398bd220c02900d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37baddf38bb569cbf2d2f186a5171767402b94fa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2.0 * (pred*targs).sum() / ((pred+targs).sum() + 1.0)\n",
    "\n",
    "def IoU(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    intersection = (pred*targs).sum()\n",
    "    return intersection / ((pred+targs).sum() - intersection + 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b3a20e5e2fdedf1439737f40c14c78f9a69e29a"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a3b8f6b454a8d8b48505fbc5964e900a59ef09e",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_base = load_pretrained(get_base(),PRETRAINED)\n",
    "m = to_gpu(Unet34(m_base))\n",
    "models = UnetModel(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d52f2e1e45ec597d901a814f592ac153168ebff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab92ee6aaa8d31587d1bc6d2cad9e0407444c5de",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sz = 256 #image size\n",
    "bs = 64  #batch size\n",
    "\n",
    "md = get_data(sz,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb1016214b0de75f56d2036241a27684b4c4bd1b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn = ConvLearner(md, models)\n",
    "learn.opt_fn=optim.Adam\n",
    "learn.crit = MixedLoss(10.0, 2.0)\n",
    "learn.metrics=[accuracy_thresh(0.5),dice,IoU]\n",
    "wd=1e-7\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d4aa02ec617bdf974fcdd77497af1ce5d9993aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d2d28f3482827318f673aee6fe3e60e2cf0ac44"
   },
   "source": [
    "Training only the decoder part for 1 epoch (15 min) leads to ~0.8 dice that outperforms all publicly available models in this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd9510104082f97119adebc00bb16750710266a2",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=1,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a06ef9bc205d434ef5be901ea489a368e24d1435",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('Unet34_256_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1288146ed507f7a6876d756e5472f8dda15cc58"
   },
   "source": [
    "Unfreeze the model and train it with differential learning rate. The lr of the head part is still 1e-3, while the middle layers of the model are trained with 1e-4 lr, and the base is trained with even smaller lr, 1e-5, since low level detectors do not vary much from one image data set to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eef154a2aa260f370c9665ec804b1abecdef3ccf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrs = np.array([lr/100,lr/10,lr])\n",
    "learn.unfreeze() #unfreeze the encoder\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8498665da79f8a147b0599142bb4034e399c38aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs,2,wds=wd,cycle_len=1,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8d9a6f6ecb66054d32e8c41d316672dbd9bb03b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs/3,2,wds=wd,cycle_len=2,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c2b287d62042002e17cf7797ae48ccd05dd36cd"
   },
   "source": [
    "The training has been run with learning rate annealing. Periodic lr increase followed by slow decrease drives the system out of steep minima (when lr is high) towards broader ones (which are explored when lr decreases) that enhances the ability of the model to generalize and reduces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fd829270a788812cb9f3f7f5fe7c6e43d934ab7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16ea8e6c7a593be13aad3cc6fe88e4cb657db35d"
   },
   "source": [
    "Saved model can be ued for further training or for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df25dc961326f3e554b9c4d38525705664ec5075",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('Unet34_256_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5f92ae7e3e348f95129a3742ad76e4a502bf6d1"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb4a8386560ebde1f2277d94ecb5ee4a1bbc50d2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Show_images(x,yp,yt):\n",
    "    columns = 3\n",
    "    rows = min(bs,8)\n",
    "    fig=plt.figure(figsize=(columns*4, rows*4))\n",
    "    for i in range(rows):\n",
    "        fig.add_subplot(rows, columns, 3*i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(x[i])\n",
    "        fig.add_subplot(rows, columns, 3*i+2)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(yp[i])\n",
    "        fig.add_subplot(rows, columns, 3*i+3)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(yt[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0206fb587e6bb9ddfc7366f6d04e47101b275514",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.model.eval();\n",
    "x,y = next(iter(md.val_dl))\n",
    "yp = to_np(F.sigmoid(learn.model(V(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cda6a85fcbe5263406777a19729d0aec890ab99",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Show_images(np.asarray(md.val_ds.denorm(x)), yp, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9850c2014ba1980bc0cca9b9a1177af5caa24da0"
   },
   "source": [
    "The results are not ideal, but almost all ships are captured correctly even if the model is making the prediction on a very low resolution (256x256) images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f0b3a89ccb06468283d8e2611c3981895aae24a"
   },
   "source": [
    "### Training (384x384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43deeffeee35bde4b11ae3a1aae13ecfabeebf4c"
   },
   "source": [
    "Fortunately, modern convolutional nets support input images of arbitrary resolution. To decrease the training time, one can start training the model on low resolution images first and continue training on higher resolution images for fewer epochs. In addition, a model pretrained on low resolution images first generalizes better since a pixel information is less available and high order features are tended to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99f0972e31576ba82335a54825cc71853e7a5afd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sz = 384 #image size\n",
    "bs = 32  #batch size\n",
    "\n",
    "md = get_data(sz,bs)\n",
    "learn.set_data(md)\n",
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2cc8a6dd9fbf4d03893088d3be3e259942089d30"
   },
   "source": [
    "Due to the kernel run time limit, the model was further trained only for two epochs on 384x384 (dice ~0.87) followed by one epoch on 768x768 images. In an independent run I trained a model on 384x384 images for 12 epochs that boosted dice to 0.89 followed by training on full resolution images that increased dice further to 0.905."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d40a6e24844002d52bb844d2a4873c164d06e39",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs/5,1,wds=wd,cycle_len=2,use_clr=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fce1cccfb4fd63853d31ae3af9f2e2897ae1e802",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('Unet34_384_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e129b20579e8dff1523f9da36e4b54bcbde9b58"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c92425b7455925580e6d03b373babaebc261047e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.model.eval();\n",
    "x,y = next(iter(md.val_dl))\n",
    "yp = to_np(F.sigmoid(learn.model(V(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "507ef05ee2301e0ebcc69a679a4eb57a99a88e4f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Show_images(np.asarray(md.val_ds.denorm(x)), yp, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a2b5a8a6d4826f8ef2ccb1b1e26c749edae3aa9"
   },
   "source": [
    "### Training (768x768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82744b9640856bc71a861240263f8d314544ebe6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sz = 768 #image size\n",
    "bs = 6  #batch size\n",
    "\n",
    "md = get_data(sz,bs)\n",
    "learn.set_data(md)\n",
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfbb04dc51276c62a97b5a02ff92dbb20b206a3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs/10,1,wds=wd,cycle_len=1,use_clr=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "addc63f66aa829048a2e285964771967d409aa93"
   },
   "source": [
    "Training for just one epoch is insufficient to achieve high dice. However, if the training is continued, the dice can reach 0.90+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6bc3ceacb7e616594fd2c8ea960e2e733ba81de",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('Unet34_768_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7584a0ba0ad6667987757dbe5cf3a3dcfea6614"
   },
   "source": [
    "And finally, I put a picture (original image, prediction, ground truth) obtained by the model with dice 0.895 trained further on full resolution images in an independent run. Apart from one tiny ship in the last image, everything is captured. When I zoomed it in, it is really not clear if it is a ship of just a small island: I see only several white pixels, and there are several small islands under the water. Another interesting thing is that the model is able to capture details that it was not trained for. In particular, in 4-th image the model captures antennas (upper right ship) and the shape of ships, even if training set is composed of pixelized bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e23a4bf43cf33af8e36b93035642b30e3853bea"
   },
   "source": [
    "![1](https://image.ibb.co/mrqdze/Ship_Detection.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
