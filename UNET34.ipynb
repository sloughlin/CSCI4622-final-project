{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b8c18e1d49e580b84b194084a08d59e2a497b54"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f0a329467823b3dc6d5f86b79f55500cc44acc8"
   },
   "source": [
    "There are two most obvious network architectures to approach this competition: U-net and SSD. Each of them has pros and cons. In particular, U-net provides a relatively simple way to solve the competition challenge using image segmentation. However, this competition requires prediction of an individual mask for each ship rather than one mask for entire image. Therefore, some creative postprocessing may be needed, especially to separate ships with overlapping masks, if it is even possible. Another drawback is that the data is labeled with using pixelized bounding boxes rather than real ship masks, therefore the score of U-net based models is lowered. Meanwhile, implementation of SSD requires usage of rotating bounding boxes (https://arxiv.org/pdf/1711.09405.pdf) that is not common and, therefore, would take additional efforts for development of the model and the corresponding loss function. In addition, bounding boxes are not provided in this competition and must be generated based on the pixel masks. Nevertheless, this approach is expected to provide higher score than U-net, especially since the data is labeled based on pixelized bounding boxes (I expect organizers used SSD with rotating bounding boxes to label train and test data).\n",
    "Since the first approach is more straightforward, I'll begin with presenting a kernel about U-net. In this post I will describe how to use pretrained ResNet34 to build a high accuracy image segmentation model. In particular, after training only a decoder for 1 epoch (15 min) on 256x256 rescaled images, the dice coefficient reaches ~0.8 (IoU ~0.67) that significantly outperforms all publicly available models posted so far in this competition. After training the entire model for 6 additional epochs with learning rate annealing, the dice coefficient reaches ~0.86 (IoU ~0.75). Due to the kernel run time limit, the model is further trained only for two epochs on 384x384 (dice ~0.87) followed by one epoch on 768x768 images. In an independent run I trained a model on 384x384 images for 12 epochs that boosted dice to 0.89 followed by training on full resolution images that increased dice further to 0.905."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai==0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/50/6d/9d0d6e17a78b0598d5e8c49a0d03ffc7ff265ae62eca3e2345fab14edb9b/fastai-0.7.0-py3-none-any.whl (112kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (2018.10.15)\n",
      "Requirement already satisfied: numpy in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (1.15.4)\n",
      "Requirement already satisfied: Pygments in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (2.2.0)\n",
      "Collecting ptyprocess (from fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: PyYAML in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (3.13)\n",
      "Requirement already satisfied: sklearn-pandas in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (1.8.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (5.1.0)\n",
      "Requirement already satisfied: cycler in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.10.0)\n",
      "Requirement already satisfied: feather-format in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.4.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (3.4.4.19)\n",
      "Requirement already satisfied: testpath in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.4.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (2.7.5)\n",
      "Requirement already satisfied: MarkupSafe in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (1.1.0)\n",
      "Requirement already satisfied: bcolz in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (1.2.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (5.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (1.1.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (4.3.0)\n",
      "Requirement already satisfied: jedi in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.13.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (2018.7)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.7.5)\n",
      "Requirement already satisfied: seaborn in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.9.0)\n",
      "Requirement already satisfied: torch<0.4 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.3.1.post2)\n",
      "Requirement already satisfied: isoweek in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (1.3.3)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (2.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.5.1)\n",
      "Requirement already satisfied: graphviz in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.10.1)\n",
      "Collecting plotnine (from fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/84/02/b171c828560aea3a5da1efda464230dac3ef4f4834b88e0bd52ad14a08f0/plotnine-0.5.1-py2.py3-none-any.whl (3.6MB)\n",
      "Requirement already satisfied: html5lib in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (1.0.1)\n",
      "Requirement already satisfied: traitlets in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (4.3.2)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (2.6.0)\n",
      "Requirement already satisfied: Jinja2 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (2.10)\n",
      "Requirement already satisfied: torchtext in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.2.3)\n",
      "Requirement already satisfied: jupyter in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (1.0.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.1.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.23.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (3.0.2)\n",
      "Requirement already satisfied: widgetsnbextension in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (3.4.2)\n",
      "Requirement already satisfied: tornado in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (4.5.3)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.2.0)\n",
      "Requirement already satisfied: simplegeneric in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (3.0.1)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.2.3)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (7.4.2)\n",
      "Requirement already satisfied: pandas-summary in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.0.5)\n",
      "Requirement already satisfied: ipython in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (7.2.0)\n",
      "Requirement already satisfied: pyzmq in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (17.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (0.2.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from fastai==0.7.0) (4.28.1)\n",
      "Requirement already satisfied: scikit-learn>=0.15.0 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from sklearn-pandas->fastai==0.7.0) (0.20.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from ipykernel->fastai==0.7.0) (5.2.3)\n",
      "Requirement already satisfied: six in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from cycler->fastai==0.7.0) (1.11.0)\n",
      "Requirement already satisfied: pyarrow>=0.4.0 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from feather-format->fastai==0.7.0) (0.11.1)\n",
      "Requirement already satisfied: parso>=0.3.0 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from jedi->fastai==0.7.0) (0.3.1)\n",
      "Requirement already satisfied: patsy>=0.4.1 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from plotnine->fastai==0.7.0) (0.5.1)\n",
      "Collecting descartes>=1.1.0 (from plotnine->fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/b6/1ed2eb03989ae574584664985367ba70cd9cf8b32ee8cad0e8aaeac819f3/descartes-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: statsmodels>=0.8.0 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from plotnine->fastai==0.7.0) (0.9.0)\n",
      "Collecting mizani>=0.5.2 (from plotnine->fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/3a/1d1c5563b6aeb5fffda694b70d649a0f728a112b79a66b85a6af4814a643/mizani-0.5.2-py2.py3-none-any.whl (58kB)\n",
      "Requirement already satisfied: requests in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from torchtext->fastai==0.7.0) (2.20.1)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from jupyter->fastai==0.7.0) (4.4.2)\n",
      "Requirement already satisfied: notebook in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from jupyter->fastai==0.7.0) (5.7.2)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from jupyter->fastai==0.7.0) (6.0.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from jupyter->fastai==0.7.0) (5.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from matplotlib->fastai==0.7.0) (1.0.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from ipywidgets->fastai==0.7.0) (4.4.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from ipython->fastai==0.7.0) (0.4.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from ipython->fastai==0.7.0) (40.6.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from ipython->fastai==0.7.0) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from ipython->fastai==0.7.0) (2.0.7)\n",
      "Requirement already satisfied: jupyter_core in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from jupyter-client->ipykernel->fastai==0.7.0) (4.4.0)\n",
      "Collecting palettable (from mizani>=0.5.2->plotnine->fastai==0.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/8a/84537c0354f0d1f03bf644b71bf8e0a50db9c1294181905721a5f3efbf66/palettable-3.1.1-py2.py3-none-any.whl (77kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from requests->torchtext->fastai==0.7.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from requests->torchtext->fastai==0.7.0) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from requests->torchtext->fastai==0.7.0) (2.7)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from notebook->jupyter->fastai==0.7.0) (0.8.1)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from notebook->jupyter->fastai==0.7.0) (0.4.2)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from notebook->jupyter->fastai==0.7.0) (1.5.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from nbconvert->jupyter->fastai==0.7.0) (1.4.2)\n",
      "Requirement already satisfied: mistune>=0.7.4 in c:\\users\\nullzunyan\\anaconda3\\envs\\fastai\\lib\\site-packages (from nbconvert->jupyter->fastai==0.7.0) (0.8.4)\n",
      "Installing collected packages: ptyprocess, descartes, palettable, mizani, plotnine, fastai\n",
      "Successfully installed descartes-1.1.0 fastai-0.7.0 mizani-0.5.2 palettable-3.1.1 plotnine-0.5.1 ptyprocess-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fastai==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ddd71a95c05902b29ce4e3768a8127c2a0b7098d"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "b93d5422dfd31f43df9cdd3e301547cc95f25980"
   },
   "outputs": [],
   "source": [
    "PATH = './'\n",
    "TRAIN = 'E:/Data/all/train_v2'\n",
    "TEST = 'E:/Data/all/test_v2'\n",
    "SEGMENTATION = 'E:/Data/all/train_ship_segmentations_v2.csv'\n",
    "PRETRAINED = 'E:/Data/all/PreTrained/Resnet34_lable_256_1.h5'\n",
    "exclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n",
    "                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n",
    "                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n",
    "                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] #corrupted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "9a72e076dc04cf1786edae26a1d2f15ec3de234a"
   },
   "outputs": [],
   "source": [
    "nw = 2   #number of workers for data loader\n",
    "arch = resnet34 #specify target architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "49199ea17e9e9ba9893c10d06c1fb419e22aeb1b"
   },
   "outputs": [],
   "source": [
    "train_names = [f for f in os.listdir(TRAIN)]\n",
    "test_names = [f for f in os.listdir(TEST)]\n",
    "for el in exclude_list:\n",
    "    if(el in train_names): train_names.remove(el)\n",
    "    if(el in test_names): test_names.remove(el)\n",
    "#5% of data in the validation set is sufficient for model evaluation\n",
    "tr_n, val_n = train_test_split(train_names, test_size=0.05, random_state=42)\n",
    "segmentation_df = pd.read_csv(os.path.join(PATH, SEGMENTATION)).set_index('ImageId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b95102ddd563191f9b15eb115f736e93c837d080"
   },
   "source": [
    "One of the challenges of this competition is strong data unbalance. Even if only images with ships are considered, the ratio of mask pixels to the total number of pixels is ~1:1000. If images with no ships are included, this ratio goes to ~1:10000, which is quite tough to handle. Therefore, I drop all images without ships, that makes the training set more balanced and also reduces the time per each epoch almost by 4 times. In an independent run, when the dice of my model reached 0.895, I ran it on images without ships and identified ~3600 false positive predictions out ~70k images. The incorrectly predicted images were incorporated to the training set as negative examples, and training was continued. The problem of false positive predictions can be further mitigated by stacking U-net model with a classification model predicting if ships are present in a particular image (https://www.kaggle.com/iafoss/fine-tuning-resnet34-on-ship-detection - ~98% accuracy). \n",
    "I also noticed that in some kernels the dataset is tried to be balanced by keeping approximately the same number of images with 0, 1, 2, etc. ships. However, this strategy would be effective for such task as ship counting rather than training U-net or SSD.  One possible way to balance the dataset is creative cropping the images that keeps approximately the same number of pixels corresponding to a ship or something else. However, I doubt that such approach will effective in this competition. Therefore, a special loss function must be used to mitigate the data unbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "6a3d8c70e03964738322ca99084f088adc1c5f3e"
   },
   "outputs": [],
   "source": [
    "def cut_empty(names):\n",
    "    return [name for name in names \n",
    "            if(type(segmentation_df.loc[name]['EncodedPixels']) != float)]\n",
    "\n",
    "tr_n = cut_empty(tr_n)\n",
    "val_n = cut_empty(val_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "505f803555b8d9d0378a73d227da4b8174f2086d"
   },
   "outputs": [],
   "source": [
    "def get_mask(img_id, df):\n",
    "    shape = (768,768)\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    masks = df.loc[img_id]['EncodedPixels']\n",
    "    if(type(masks) == float): return img.reshape(shape)\n",
    "    if(type(masks) == str): masks = [masks]\n",
    "    for mask in masks:\n",
    "        s = mask.split()\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i]) - 1\n",
    "            length = int(s[2*i+1])\n",
    "            img[start:start+length] = 1\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "f67a326e1269bffca392fcf4ac10bf4a532ca56b"
   },
   "outputs": [],
   "source": [
    "class pdFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, path, transform):\n",
    "        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n",
    "        super().__init__(fnames, transform, path)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        img = open_image(os.path.join(self.path, self.fnames[i]))\n",
    "        if self.sz == 768: return img \n",
    "        else: return cv2.resize(img, (self.sz, self.sz))\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        mask = np.zeros((768,768), dtype=np.uint8) if (self.path == TEST) \\\n",
    "            else get_mask(self.fnames[i], self.segmentation_df)\n",
    "        img = Image.fromarray(mask).resize((self.sz, self.sz)).convert('RGB')\n",
    "        return np.array(img).astype(np.float32)\n",
    "    \n",
    "    def get_c(self): return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f51da2928183042aa99adfe9d1da494f6c624e57"
   },
   "source": [
    "The carrently availible on kaggle version of fastai has a bug in RandomLighting data agmentation class. It would be nice if kaggle updated fastai version to the last one, where this and other bugs are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "649601eb0bd110b02f15349cc9d26d60ddeee953"
   },
   "outputs": [],
   "source": [
    "class RandomLighting(Transform):\n",
    "    def __init__(self, b, c, tfm_y=TfmType.NO):\n",
    "        super().__init__(tfm_y)\n",
    "        self.b,self.c = b,c\n",
    "\n",
    "    def set_state(self):\n",
    "        self.store.b_rand = rand0(self.b)\n",
    "        self.store.c_rand = rand0(self.c)\n",
    "\n",
    "    def do_transform(self, x, is_y):\n",
    "        if is_y and self.tfm_y != TfmType.PIXEL: return x  #add this line to fix the bug\n",
    "        b = self.store.b_rand\n",
    "        c = self.store.c_rand\n",
    "        c = -1/(c-1) if c<0 else c+1\n",
    "        x = lighting(x, b, c)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "835b75b3508f418bac53ee6725694af208e1f28c"
   },
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    #data augmentation\n",
    "    aug_tfms = [RandomRotate(20, tfm_y=TfmType.CLASS),\n",
    "                RandomDihedral(tfm_y=TfmType.CLASS),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.CLASS)]\n",
    "    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS, \n",
    "                aug_tfms=aug_tfms)\n",
    "    tr_names = tr_n if (len(tr_n)%bs == 0) else tr_n[:-(len(tr_n)%bs)] #cut incomplete batch\n",
    "    ds = ImageData.get_ds(pdFilesDataset, (tr_names,TRAIN), \n",
    "                (val_n,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    #md.is_multi = False\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b77a79e1e1e3f4c8a9e7120feb9a843b41492f8",
    "collapsed": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc66537bd5382d9e874cfa8c0616965e8d9f5f7d"
   },
   "source": [
    "The model used in this kernel is inspired by a Carvana example from FastAI course (http://course.fast.ai/index.html). It is composed of a ResNet34 based encoder and a simple upsampling decoder. Similar to the original U-net, skip connections are added between encoder and decoder to facilitate the information flow at different detalization levels. Meanwhile, using a pretrained ResNet34 model allows us to have a powerful encoder capable of handling elaborated feature, in comparison with the original U-net, without a risk of overfitting and necessity of training a big model from scratch. The total capacity of the model is ~21M parameters. Before using, the original ResNet34 model was further fine-tuned on ship/no-ship classification task (https://www.kaggle.com/iafoss/fine-tuning-resnet34-on-ship-detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "5bfefb757e1d40da7d8761c824170214a4bec08b"
   },
   "outputs": [],
   "source": [
    "cut,lr_cut = model_meta[arch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "7b0649b17721919f475a99f41f47f09c4fcc41d6"
   },
   "outputs": [],
   "source": [
    "def get_base():                   #load ResNet34 model\n",
    "    layers = cut_model(arch(True), cut)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def load_pretrained(model, path): #load a model pretrained on ship/no-ship classification\n",
    "    weights = torch.load(PRETRAINED, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "77ca73006aec0c9dfe4be6deb2d6cf524840cd62"
   },
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p))\n",
    "\n",
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()\n",
    "    \n",
    "class Unet34(nn.Module):\n",
    "    def __init__(self, rn):\n",
    "        super().__init__()\n",
    "        self.rn = rn\n",
    "        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x)\n",
    "        return x[:,0]\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()\n",
    "            \n",
    "class UnetModel():\n",
    "    def __init__(self,model,name='Unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n",
    "        return lgs + [children(self.model)[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c71a3af2a83b1122902a3f39e6fda1af5afbb99",
    "collapsed": true
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19d0fe1a3e00cea135163b5540338e2744615c3d"
   },
   "source": [
    "Loss function is one of the most crucial parts of the completion. Due to strong data unbalance, simple loss functions, such as Binary Cross-Entropy loss, do not really work. Soft dice loss can be helpful since it boosts prediction of correct masks, but it leads to unstable training. Winners of image segmentation challenges typically combine BCE loss with dice (http://blog.kaggle.com/2017/12/22/carvana-image-masking-first-place-interview/). Similar loss function is used in publicly available models in this completion. I would agree that this combined loss function works perfectly for Carvana completion, where the number of pixels in the mask is about half of the total number of pixels. However, 1:1000 pixel unbalance deteriorates training with BCE. \n",
    "If one tries to recall what is the loss function that should be used for strongly unbalanced data set, it is focal loss (https://arxiv.org/pdf/1708.02002.pdf), which revolutionized one stage object localization method in 2017. This loss function demonstrates amazing results on datasets with unbalance level 1:10-1000. In addition to focal loss, I include -log(soft dice loss). Log is important in the convex of the current competition since it boosts the loss for the cases when objects are not detected correctly and dice is close to zero. It allows to avoid false negative predictions or completely incorrect masks for images with one ship (the major part of the training set). Also, since the loss for such objects is very high, the model more effectively incorporates the knowledge about such objects and handles them even in images with multiple ships. To bring two losses to similar scale, focal loss is multiplied by 10. The implementation of focal loss is borrowed from https://becominghuman.ai/investigating-focal-and-dice-loss-for-the-kaggle-2018-data-science-bowl-65fb9af4f36c ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "8de4222fd28596dbae5d55f8172ac28271f678f2"
   },
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "456acf8ffe7b80cd94eb73bfaeb6b1061bbb44c7"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "1cfb00a50821861c69534ee7398bd220c02900d8"
   },
   "outputs": [],
   "source": [
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "37baddf38bb569cbf2d2f186a5171767402b94fa"
   },
   "outputs": [],
   "source": [
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2.0 * (pred*targs).sum() / ((pred+targs).sum() + 1.0)\n",
    "\n",
    "def IoU(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    intersection = (pred*targs).sum()\n",
    "    return intersection / ((pred+targs).sum() - intersection + 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b3a20e5e2fdedf1439737f40c14c78f9a69e29a"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "1a3b8f6b454a8d8b48505fbc5964e900a59ef09e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_base = load_pretrained(get_base(),PRETRAINED)\n",
    "m = to_gpu(Unet34(m_base))\n",
    "models = UnetModel(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "3d52f2e1e45ec597d901a814f592ac153168ebff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet34(\n",
       "  (rn): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): UnetBlock(\n",
       "    (x_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (tr_conv): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (up2): UnetBlock(\n",
       "    (x_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (tr_conv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (up3): UnetBlock(\n",
       "    (x_conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (tr_conv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (up4): UnetBlock(\n",
       "    (x_conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (tr_conv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (up5): ConvTranspose2d(256, 1, kernel_size=(2, 2), stride=(2, 2))\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "ab92ee6aaa8d31587d1bc6d2cad9e0407444c5de"
   },
   "outputs": [],
   "source": [
    "sz = 256 #image size\n",
    "bs = 64  #batch size\n",
    "\n",
    "md = get_data(sz,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "eb1016214b0de75f56d2036241a27684b4c4bd1b"
   },
   "outputs": [],
   "source": [
    "learn = ConvLearner(md, models)\n",
    "learn.opt_fn=optim.Adam\n",
    "learn.crit = MixedLoss(10.0, 2.0)\n",
    "learn.metrics=[accuracy_thresh(0.5),dice,IoU]\n",
    "wd=1e-7\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "4d4aa02ec617bdf974fcdd77497af1ce5d9993aa"
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d2d28f3482827318f673aee6fe3e60e2cf0ac44"
   },
   "source": [
    "Training only the decoder part for 1 epoch (15 min) leads to ~0.8 dice that outperforms all publicly available models in this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "fd9510104082f97119adebc00bb16750710266a2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0037b2ad62db494cb218f9a9c9b4e273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initialâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/631 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at c:\\anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\thc\\generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-2463fb9d1790>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcycle_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muse_clr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[1;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\model.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, xs, y, epoch)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mxtra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mxtra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-6f07a9c27b0c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-6f07a9c27b0c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, up_p, x_p)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mup_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mup_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mup_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mx_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mcat_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mup_p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_p\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 282\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at c:\\anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\thc\\generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=1,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a06ef9bc205d434ef5be901ea489a368e24d1435",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('Unet34_256_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1288146ed507f7a6876d756e5472f8dda15cc58"
   },
   "source": [
    "Unfreeze the model and train it with differential learning rate. The lr of the head part is still 1e-3, while the middle layers of the model are trained with 1e-4 lr, and the base is trained with even smaller lr, 1e-5, since low level detectors do not vary much from one image data set to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eef154a2aa260f370c9665ec804b1abecdef3ccf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrs = np.array([lr/100,lr/10,lr])\n",
    "learn.unfreeze() #unfreeze the encoder\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8498665da79f8a147b0599142bb4034e399c38aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs,2,wds=wd,cycle_len=1,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8d9a6f6ecb66054d32e8c41d316672dbd9bb03b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs/3,2,wds=wd,cycle_len=2,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c2b287d62042002e17cf7797ae48ccd05dd36cd"
   },
   "source": [
    "The training has been run with learning rate annealing. Periodic lr increase followed by slow decrease drives the system out of steep minima (when lr is high) towards broader ones (which are explored when lr decreases) that enhances the ability of the model to generalize and reduces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fd829270a788812cb9f3f7f5fe7c6e43d934ab7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16ea8e6c7a593be13aad3cc6fe88e4cb657db35d"
   },
   "source": [
    "Saved model can be ued for further training or for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df25dc961326f3e554b9c4d38525705664ec5075",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('Unet34_256_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5f92ae7e3e348f95129a3742ad76e4a502bf6d1"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb4a8386560ebde1f2277d94ecb5ee4a1bbc50d2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Show_images(x,yp,yt):\n",
    "    columns = 3\n",
    "    rows = min(bs,8)\n",
    "    fig=plt.figure(figsize=(columns*4, rows*4))\n",
    "    for i in range(rows):\n",
    "        fig.add_subplot(rows, columns, 3*i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(x[i])\n",
    "        fig.add_subplot(rows, columns, 3*i+2)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(yp[i])\n",
    "        fig.add_subplot(rows, columns, 3*i+3)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(yt[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0206fb587e6bb9ddfc7366f6d04e47101b275514",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.model.eval();\n",
    "x,y = next(iter(md.val_dl))\n",
    "yp = to_np(F.sigmoid(learn.model(V(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cda6a85fcbe5263406777a19729d0aec890ab99",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Show_images(np.asarray(md.val_ds.denorm(x)), yp, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9850c2014ba1980bc0cca9b9a1177af5caa24da0"
   },
   "source": [
    "The results are not ideal, but almost all ships are captured correctly even if the model is making the prediction on a very low resolution (256x256) images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f0b3a89ccb06468283d8e2611c3981895aae24a"
   },
   "source": [
    "### Training (384x384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43deeffeee35bde4b11ae3a1aae13ecfabeebf4c"
   },
   "source": [
    "Fortunately, modern convolutional nets support input images of arbitrary resolution. To decrease the training time, one can start training the model on low resolution images first and continue training on higher resolution images for fewer epochs. In addition, a model pretrained on low resolution images first generalizes better since a pixel information is less available and high order features are tended to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99f0972e31576ba82335a54825cc71853e7a5afd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sz = 384 #image size\n",
    "bs = 32  #batch size\n",
    "\n",
    "md = get_data(sz,bs)\n",
    "learn.set_data(md)\n",
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2cc8a6dd9fbf4d03893088d3be3e259942089d30"
   },
   "source": [
    "Due to the kernel run time limit, the model was further trained only for two epochs on 384x384 (dice ~0.87) followed by one epoch on 768x768 images. In an independent run I trained a model on 384x384 images for 12 epochs that boosted dice to 0.89 followed by training on full resolution images that increased dice further to 0.905."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d40a6e24844002d52bb844d2a4873c164d06e39",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs/5,1,wds=wd,cycle_len=2,use_clr=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fce1cccfb4fd63853d31ae3af9f2e2897ae1e802",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('Unet34_384_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e129b20579e8dff1523f9da36e4b54bcbde9b58"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c92425b7455925580e6d03b373babaebc261047e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.model.eval();\n",
    "x,y = next(iter(md.val_dl))\n",
    "yp = to_np(F.sigmoid(learn.model(V(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "507ef05ee2301e0ebcc69a679a4eb57a99a88e4f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Show_images(np.asarray(md.val_ds.denorm(x)), yp, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a2b5a8a6d4826f8ef2ccb1b1e26c749edae3aa9"
   },
   "source": [
    "### Training (768x768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82744b9640856bc71a861240263f8d314544ebe6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sz = 768 #image size\n",
    "bs = 6  #batch size\n",
    "\n",
    "md = get_data(sz,bs)\n",
    "learn.set_data(md)\n",
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfbb04dc51276c62a97b5a02ff92dbb20b206a3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs/10,1,wds=wd,cycle_len=1,use_clr=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "addc63f66aa829048a2e285964771967d409aa93"
   },
   "source": [
    "Training for just one epoch is insufficient to achieve high dice. However, if the training is continued, the dice can reach 0.90+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6bc3ceacb7e616594fd2c8ea960e2e733ba81de",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save('Unet34_768_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7584a0ba0ad6667987757dbe5cf3a3dcfea6614"
   },
   "source": [
    "And finally, I put a picture (original image, prediction, ground truth) obtained by the model with dice 0.895 trained further on full resolution images in an independent run. Apart from one tiny ship in the last image, everything is captured. When I zoomed it in, it is really not clear if it is a ship of just a small island: I see only several white pixels, and there are several small islands under the water. Another interesting thing is that the model is able to capture details that it was not trained for. In particular, in 4-th image the model captures antennas (upper right ship) and the shape of ships, even if training set is composed of pixelized bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e23a4bf43cf33af8e36b93035642b30e3853bea"
   },
   "source": [
    "![1](https://image.ibb.co/mrqdze/Ship_Detection.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
